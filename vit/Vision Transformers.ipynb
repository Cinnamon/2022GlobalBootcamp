{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c8f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e869684",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b3e997",
   "metadata": {},
   "source": [
    "![](assets/attn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "941d86a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAtt(nn.Module):\n",
    "    \"\"\"Basic attention block\n",
    "    \n",
    "    This is a simplified version referenced from: \n",
    "    https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py#L178-L202\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, head=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_head = d_model // head\n",
    "        self.head = head\n",
    "        \n",
    "        # We don't want to create *head* instances of Linear class\n",
    "        # so we just group it to single Linear that takes in *d_model* channels and returns *d_head* x *head* channels\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        nn.init.xavier_uniform_(self.W_q.weight)\n",
    "        \n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        nn.init.xavier_uniform_(self.W_k.weight)\n",
    "        \n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        nn.init.xavier_uniform_(self.W_v.weight)\n",
    "        \n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        nn.init.xavier_uniform_(self.W_o.weight)\n",
    "        nn.init.zeros_(self.W_o.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: a B x N x C tensor\n",
    "        \n",
    "        Annotations:\n",
    "        - B: batch size\n",
    "        - N: number of token\n",
    "        - C: number of channel\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        queries = self.W_q(x) # B x N x head*d_head\n",
    "        keys = self.W_k(x)\n",
    "        values = self.W_v(x)\n",
    "        \n",
    "        queries = queries.reshape(B, N, self.head, self.d_head).permute(0, 2, 1, 3) # B x head x N x d_head\n",
    "        keys = keys.reshape(B, N, self.head, self.d_head).permute(0, 2, 1, 3)\n",
    "        values = values.reshape(B, N, self.head, self.d_head).permute(0, 2, 1, 3)\n",
    "        \n",
    "        attn = (queries @ keys.transpose(-2, -1)) / self.d_head ** 0.5\n",
    "        attn = F.softmax(attn, dim=-1) # B x head x N x N\n",
    "        \n",
    "        x = attn @ values # B x head x N x h_head\n",
    "        x = x.transpose(1, 2) # B x N x head x h_head\n",
    "        x = x.reshape(B, N, C) # B x N x head*h_head - Remind: d_model = C = head*h_head\n",
    "        \n",
    "        x = self.W_o(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba2edcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "module = MultiHeadAtt(256)\n",
    "input_tensor = torch.ones((1, 16, 256))\n",
    "\n",
    "output_tensor = module(input_tensor)\n",
    "assert output_tensor.shape == (1, 16, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585d7f1f",
   "metadata": {},
   "source": [
    "![](assets/vit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa0b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \"\"\"MLP or Feed forward network used in attention blocks\"\"\"\n",
    "    def __init__(self, d_in_out, d_hidden):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_in_out, d_hidden)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        \n",
    "        self.fc2 = nn.Linear(d_hidden, d_in_out)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: a B x N x C tensor\n",
    "        \n",
    "        Annotations:\n",
    "        - B: batch size\n",
    "        - N: number of token\n",
    "        - C: number of channel\n",
    "        \"\"\"\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7183096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Basic building block of ViT\"\"\"\n",
    "    def __init__(self, d_model, head, d_ff_hid):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_norm = nn.LayerNorm(d_model)\n",
    "        self.multi_attn = MultiHeadAtt(d_model, head)\n",
    "        \n",
    "        self.ff_norm = nn.LayerNorm(d_model)\n",
    "        self.ff = FFN(d_model, d_ff_hid)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: a B x N x C tensor\n",
    "        \n",
    "        Annotations:\n",
    "        - B: batch size\n",
    "        - N: number of token\n",
    "        - C: number of channel\n",
    "        \"\"\"\n",
    "        x_res = x\n",
    "        x = self.input_norm(x)\n",
    "        z = self.multi_attn(x) + x_res\n",
    "        \n",
    "        z_res = z\n",
    "        z = self.ff_norm(z)\n",
    "        z = self.ff(z) + z_res\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23b42173",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"A layer that splits image into patches and using a CNN to compute embedding feature of each patch\"\"\"\n",
    "    def __init__(self, img_size, patch_size, img_c, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_size = img_size if isinstance(img_size, tuple) else (img_size, img_size)\n",
    "        self.patch_size = patch_size if isinstance(patch_size, tuple) else (patch_size, patch_size)\n",
    "        \n",
    "        self.grid_size = (self.img_size[0] // self.patch_size[0], self.img_size[1] // self.patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        \n",
    "        self.conv = nn.Conv2d(img_c, d_model, kernel_size=self.patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: a B x C x H x W image tensor\n",
    "        \n",
    "        Annotations:\n",
    "        - B: batch size\n",
    "        - C: number of channel\n",
    "        - H: height\n",
    "        - W: width\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        \n",
    "        x = self.conv(x) # B x d_model x grid_H x grid_W\n",
    "        x = torch.flatten(x, 2) # B x d_model x N\n",
    "        x = x.transpose(1, 2) # B x N x d_model\n",
    "        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aab7ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "module = PatchEmbed(img_size=224, patch_size=16, img_c=3, d_model=256)\n",
    "input_tensor = torch.ones((1, 3, 224, 224))\n",
    "\n",
    "output_tensor = module(input_tensor)\n",
    "assert output_tensor.shape == (1, 14 * 14, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1ab4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \"\"\"A skeleton of a typical ViT model\"\"\"\n",
    "    def __init__(self, img_size, patch_size, img_c, d_model, num_class, encoders):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        self.patch_embeder = PatchEmbed(img_size, patch_size, img_c, d_model)\n",
    "        \n",
    "        num_patches = self.patch_embeder.num_patches\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, d_model))\n",
    "        \n",
    "        self.encoders = encoders\n",
    "        self.mlp_head = nn.Linear(d_model, num_class)\n",
    "        self.cls_morm = nn.LayerNorm(num_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: a B x C x H x W image tensor\n",
    "        \n",
    "        Annotations:\n",
    "        - B: batch size\n",
    "        - C: number of channel\n",
    "        - H: height\n",
    "        - W: width\n",
    "        \"\"\"\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x_embed = self.patch_embeder(x)\n",
    "        x_embed = torch.cat([cls_token, x_embed], dim=1)\n",
    "        x_embed = x_embed + self.pos_embed\n",
    "        \n",
    "        x_transformed = self.encoders(x_embed)\n",
    "        cls_transformed_token = x_transformed[:, 0, :] \n",
    "        cls_logits = self.mlp_head(cls_transformed_token)\n",
    "        cls_logits = self.cls_morm(cls_logits)\n",
    "        return cls_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60b04adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "        PatchEmbed-2             [-1, 196, 768]               0\n",
      "         LayerNorm-3             [-1, 197, 768]           1,536\n",
      "            Linear-4             [-1, 197, 768]         589,824\n",
      "            Linear-5             [-1, 197, 768]         589,824\n",
      "            Linear-6             [-1, 197, 768]         589,824\n",
      "            Linear-7             [-1, 197, 768]         590,592\n",
      "      MultiHeadAtt-8             [-1, 197, 768]               0\n",
      "         LayerNorm-9             [-1, 197, 768]           1,536\n",
      "           Linear-10            [-1, 197, 3072]       2,362,368\n",
      "           Linear-11             [-1, 197, 768]       2,360,064\n",
      "              FFN-12             [-1, 197, 768]               0\n",
      "TransformerEncoder-13             [-1, 197, 768]               0\n",
      "        LayerNorm-14             [-1, 197, 768]           1,536\n",
      "           Linear-15             [-1, 197, 768]         589,824\n",
      "           Linear-16             [-1, 197, 768]         589,824\n",
      "           Linear-17             [-1, 197, 768]         589,824\n",
      "           Linear-18             [-1, 197, 768]         590,592\n",
      "     MultiHeadAtt-19             [-1, 197, 768]               0\n",
      "        LayerNorm-20             [-1, 197, 768]           1,536\n",
      "           Linear-21            [-1, 197, 3072]       2,362,368\n",
      "           Linear-22             [-1, 197, 768]       2,360,064\n",
      "              FFN-23             [-1, 197, 768]               0\n",
      "TransformerEncoder-24             [-1, 197, 768]               0\n",
      "        LayerNorm-25             [-1, 197, 768]           1,536\n",
      "           Linear-26             [-1, 197, 768]         589,824\n",
      "           Linear-27             [-1, 197, 768]         589,824\n",
      "           Linear-28             [-1, 197, 768]         589,824\n",
      "           Linear-29             [-1, 197, 768]         590,592\n",
      "     MultiHeadAtt-30             [-1, 197, 768]               0\n",
      "        LayerNorm-31             [-1, 197, 768]           1,536\n",
      "           Linear-32            [-1, 197, 3072]       2,362,368\n",
      "           Linear-33             [-1, 197, 768]       2,360,064\n",
      "              FFN-34             [-1, 197, 768]               0\n",
      "TransformerEncoder-35             [-1, 197, 768]               0\n",
      "        LayerNorm-36             [-1, 197, 768]           1,536\n",
      "           Linear-37             [-1, 197, 768]         589,824\n",
      "           Linear-38             [-1, 197, 768]         589,824\n",
      "           Linear-39             [-1, 197, 768]         589,824\n",
      "           Linear-40             [-1, 197, 768]         590,592\n",
      "     MultiHeadAtt-41             [-1, 197, 768]               0\n",
      "        LayerNorm-42             [-1, 197, 768]           1,536\n",
      "           Linear-43            [-1, 197, 3072]       2,362,368\n",
      "           Linear-44             [-1, 197, 768]       2,360,064\n",
      "              FFN-45             [-1, 197, 768]               0\n",
      "TransformerEncoder-46             [-1, 197, 768]               0\n",
      "        LayerNorm-47             [-1, 197, 768]           1,536\n",
      "           Linear-48             [-1, 197, 768]         589,824\n",
      "           Linear-49             [-1, 197, 768]         589,824\n",
      "           Linear-50             [-1, 197, 768]         589,824\n",
      "           Linear-51             [-1, 197, 768]         590,592\n",
      "     MultiHeadAtt-52             [-1, 197, 768]               0\n",
      "        LayerNorm-53             [-1, 197, 768]           1,536\n",
      "           Linear-54            [-1, 197, 3072]       2,362,368\n",
      "           Linear-55             [-1, 197, 768]       2,360,064\n",
      "              FFN-56             [-1, 197, 768]               0\n",
      "TransformerEncoder-57             [-1, 197, 768]               0\n",
      "        LayerNorm-58             [-1, 197, 768]           1,536\n",
      "           Linear-59             [-1, 197, 768]         589,824\n",
      "           Linear-60             [-1, 197, 768]         589,824\n",
      "           Linear-61             [-1, 197, 768]         589,824\n",
      "           Linear-62             [-1, 197, 768]         590,592\n",
      "     MultiHeadAtt-63             [-1, 197, 768]               0\n",
      "        LayerNorm-64             [-1, 197, 768]           1,536\n",
      "           Linear-65            [-1, 197, 3072]       2,362,368\n",
      "           Linear-66             [-1, 197, 768]       2,360,064\n",
      "              FFN-67             [-1, 197, 768]               0\n",
      "TransformerEncoder-68             [-1, 197, 768]               0\n",
      "        LayerNorm-69             [-1, 197, 768]           1,536\n",
      "           Linear-70             [-1, 197, 768]         589,824\n",
      "           Linear-71             [-1, 197, 768]         589,824\n",
      "           Linear-72             [-1, 197, 768]         589,824\n",
      "           Linear-73             [-1, 197, 768]         590,592\n",
      "     MultiHeadAtt-74             [-1, 197, 768]               0\n",
      "        LayerNorm-75             [-1, 197, 768]           1,536\n",
      "           Linear-76            [-1, 197, 3072]       2,362,368\n",
      "           Linear-77             [-1, 197, 768]       2,360,064\n",
      "              FFN-78             [-1, 197, 768]               0\n",
      "TransformerEncoder-79             [-1, 197, 768]               0\n",
      "        LayerNorm-80             [-1, 197, 768]           1,536\n",
      "           Linear-81             [-1, 197, 768]         589,824\n",
      "           Linear-82             [-1, 197, 768]         589,824\n",
      "           Linear-83             [-1, 197, 768]         589,824\n",
      "           Linear-84             [-1, 197, 768]         590,592\n",
      "     MultiHeadAtt-85             [-1, 197, 768]               0\n",
      "        LayerNorm-86             [-1, 197, 768]           1,536\n",
      "           Linear-87            [-1, 197, 3072]       2,362,368\n",
      "           Linear-88             [-1, 197, 768]       2,360,064\n",
      "              FFN-89             [-1, 197, 768]               0\n",
      "TransformerEncoder-90             [-1, 197, 768]               0\n",
      "        LayerNorm-91             [-1, 197, 768]           1,536\n",
      "           Linear-92             [-1, 197, 768]         589,824\n",
      "           Linear-93             [-1, 197, 768]         589,824\n",
      "           Linear-94             [-1, 197, 768]         589,824\n",
      "           Linear-95             [-1, 197, 768]         590,592\n",
      "     MultiHeadAtt-96             [-1, 197, 768]               0\n",
      "        LayerNorm-97             [-1, 197, 768]           1,536\n",
      "           Linear-98            [-1, 197, 3072]       2,362,368\n",
      "           Linear-99             [-1, 197, 768]       2,360,064\n",
      "             FFN-100             [-1, 197, 768]               0\n",
      "TransformerEncoder-101             [-1, 197, 768]               0\n",
      "       LayerNorm-102             [-1, 197, 768]           1,536\n",
      "          Linear-103             [-1, 197, 768]         589,824\n",
      "          Linear-104             [-1, 197, 768]         589,824\n",
      "          Linear-105             [-1, 197, 768]         589,824\n",
      "          Linear-106             [-1, 197, 768]         590,592\n",
      "    MultiHeadAtt-107             [-1, 197, 768]               0\n",
      "       LayerNorm-108             [-1, 197, 768]           1,536\n",
      "          Linear-109            [-1, 197, 3072]       2,362,368\n",
      "          Linear-110             [-1, 197, 768]       2,360,064\n",
      "             FFN-111             [-1, 197, 768]               0\n",
      "TransformerEncoder-112             [-1, 197, 768]               0\n",
      "       LayerNorm-113             [-1, 197, 768]           1,536\n",
      "          Linear-114             [-1, 197, 768]         589,824\n",
      "          Linear-115             [-1, 197, 768]         589,824\n",
      "          Linear-116             [-1, 197, 768]         589,824\n",
      "          Linear-117             [-1, 197, 768]         590,592\n",
      "    MultiHeadAtt-118             [-1, 197, 768]               0\n",
      "       LayerNorm-119             [-1, 197, 768]           1,536\n",
      "          Linear-120            [-1, 197, 3072]       2,362,368\n",
      "          Linear-121             [-1, 197, 768]       2,360,064\n",
      "             FFN-122             [-1, 197, 768]               0\n",
      "TransformerEncoder-123             [-1, 197, 768]               0\n",
      "       LayerNorm-124             [-1, 197, 768]           1,536\n",
      "          Linear-125             [-1, 197, 768]         589,824\n",
      "          Linear-126             [-1, 197, 768]         589,824\n",
      "          Linear-127             [-1, 197, 768]         589,824\n",
      "          Linear-128             [-1, 197, 768]         590,592\n",
      "    MultiHeadAtt-129             [-1, 197, 768]               0\n",
      "       LayerNorm-130             [-1, 197, 768]           1,536\n",
      "          Linear-131            [-1, 197, 3072]       2,362,368\n",
      "          Linear-132             [-1, 197, 768]       2,360,064\n",
      "             FFN-133             [-1, 197, 768]               0\n",
      "TransformerEncoder-134             [-1, 197, 768]               0\n",
      "          Linear-135                   [-1, 10]           7,690\n",
      "       LayerNorm-136                   [-1, 10]              20\n",
      "================================================================\n",
      "Total params: 85,625,118\n",
      "Trainable params: 85,625,118\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 196.22\n",
      "Params size (MB): 326.63\n",
      "Estimated Total Size (MB): 523.43\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ViT variants -- ViT-B/16\n",
    "n_class = 10\n",
    "n_layer = 12\n",
    "d_model = 768\n",
    "n_head = 12\n",
    "mlp_hidden = 3072\n",
    "\n",
    "vit_encoder = nn.Sequential(*[TransformerEncoder(d_model=d_model, head=n_head, d_ff_hid=mlp_hidden) for i in range(n_layer)])\n",
    "vit_b = ViT(img_size=224, patch_size=16, img_c=3, d_model=d_model, num_class=n_class, encoders=vit_encoder)\n",
    "\n",
    "input_tensor = torch.ones((1, 3, 224, 224))\n",
    "\n",
    "output_tensor = vit_b(input_tensor)\n",
    "assert output_tensor.shape == (1, n_class), output_tensor.shape\n",
    "summary(vit_b, input_size=(3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f491c3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 1024, 14, 14]         787,456\n",
      "        PatchEmbed-2            [-1, 196, 1024]               0\n",
      "         LayerNorm-3            [-1, 197, 1024]           2,048\n",
      "            Linear-4            [-1, 197, 1024]       1,048,576\n",
      "            Linear-5            [-1, 197, 1024]       1,048,576\n",
      "            Linear-6            [-1, 197, 1024]       1,048,576\n",
      "            Linear-7            [-1, 197, 1024]       1,049,600\n",
      "      MultiHeadAtt-8            [-1, 197, 1024]               0\n",
      "         LayerNorm-9            [-1, 197, 1024]           2,048\n",
      "           Linear-10            [-1, 197, 4096]       4,198,400\n",
      "           Linear-11            [-1, 197, 1024]       4,195,328\n",
      "              FFN-12            [-1, 197, 1024]               0\n",
      "TransformerEncoder-13            [-1, 197, 1024]               0\n",
      "        LayerNorm-14            [-1, 197, 1024]           2,048\n",
      "           Linear-15            [-1, 197, 1024]       1,048,576\n",
      "           Linear-16            [-1, 197, 1024]       1,048,576\n",
      "           Linear-17            [-1, 197, 1024]       1,048,576\n",
      "           Linear-18            [-1, 197, 1024]       1,049,600\n",
      "     MultiHeadAtt-19            [-1, 197, 1024]               0\n",
      "        LayerNorm-20            [-1, 197, 1024]           2,048\n",
      "           Linear-21            [-1, 197, 4096]       4,198,400\n",
      "           Linear-22            [-1, 197, 1024]       4,195,328\n",
      "              FFN-23            [-1, 197, 1024]               0\n",
      "TransformerEncoder-24            [-1, 197, 1024]               0\n",
      "        LayerNorm-25            [-1, 197, 1024]           2,048\n",
      "           Linear-26            [-1, 197, 1024]       1,048,576\n",
      "           Linear-27            [-1, 197, 1024]       1,048,576\n",
      "           Linear-28            [-1, 197, 1024]       1,048,576\n",
      "           Linear-29            [-1, 197, 1024]       1,049,600\n",
      "     MultiHeadAtt-30            [-1, 197, 1024]               0\n",
      "        LayerNorm-31            [-1, 197, 1024]           2,048\n",
      "           Linear-32            [-1, 197, 4096]       4,198,400\n",
      "           Linear-33            [-1, 197, 1024]       4,195,328\n",
      "              FFN-34            [-1, 197, 1024]               0\n",
      "TransformerEncoder-35            [-1, 197, 1024]               0\n",
      "        LayerNorm-36            [-1, 197, 1024]           2,048\n",
      "           Linear-37            [-1, 197, 1024]       1,048,576\n",
      "           Linear-38            [-1, 197, 1024]       1,048,576\n",
      "           Linear-39            [-1, 197, 1024]       1,048,576\n",
      "           Linear-40            [-1, 197, 1024]       1,049,600\n",
      "     MultiHeadAtt-41            [-1, 197, 1024]               0\n",
      "        LayerNorm-42            [-1, 197, 1024]           2,048\n",
      "           Linear-43            [-1, 197, 4096]       4,198,400\n",
      "           Linear-44            [-1, 197, 1024]       4,195,328\n",
      "              FFN-45            [-1, 197, 1024]               0\n",
      "TransformerEncoder-46            [-1, 197, 1024]               0\n",
      "        LayerNorm-47            [-1, 197, 1024]           2,048\n",
      "           Linear-48            [-1, 197, 1024]       1,048,576\n",
      "           Linear-49            [-1, 197, 1024]       1,048,576\n",
      "           Linear-50            [-1, 197, 1024]       1,048,576\n",
      "           Linear-51            [-1, 197, 1024]       1,049,600\n",
      "     MultiHeadAtt-52            [-1, 197, 1024]               0\n",
      "        LayerNorm-53            [-1, 197, 1024]           2,048\n",
      "           Linear-54            [-1, 197, 4096]       4,198,400\n",
      "           Linear-55            [-1, 197, 1024]       4,195,328\n",
      "              FFN-56            [-1, 197, 1024]               0\n",
      "TransformerEncoder-57            [-1, 197, 1024]               0\n",
      "        LayerNorm-58            [-1, 197, 1024]           2,048\n",
      "           Linear-59            [-1, 197, 1024]       1,048,576\n",
      "           Linear-60            [-1, 197, 1024]       1,048,576\n",
      "           Linear-61            [-1, 197, 1024]       1,048,576\n",
      "           Linear-62            [-1, 197, 1024]       1,049,600\n",
      "     MultiHeadAtt-63            [-1, 197, 1024]               0\n",
      "        LayerNorm-64            [-1, 197, 1024]           2,048\n",
      "           Linear-65            [-1, 197, 4096]       4,198,400\n",
      "           Linear-66            [-1, 197, 1024]       4,195,328\n",
      "              FFN-67            [-1, 197, 1024]               0\n",
      "TransformerEncoder-68            [-1, 197, 1024]               0\n",
      "        LayerNorm-69            [-1, 197, 1024]           2,048\n",
      "           Linear-70            [-1, 197, 1024]       1,048,576\n",
      "           Linear-71            [-1, 197, 1024]       1,048,576\n",
      "           Linear-72            [-1, 197, 1024]       1,048,576\n",
      "           Linear-73            [-1, 197, 1024]       1,049,600\n",
      "     MultiHeadAtt-74            [-1, 197, 1024]               0\n",
      "        LayerNorm-75            [-1, 197, 1024]           2,048\n",
      "           Linear-76            [-1, 197, 4096]       4,198,400\n",
      "           Linear-77            [-1, 197, 1024]       4,195,328\n",
      "              FFN-78            [-1, 197, 1024]               0\n",
      "TransformerEncoder-79            [-1, 197, 1024]               0\n",
      "        LayerNorm-80            [-1, 197, 1024]           2,048\n",
      "           Linear-81            [-1, 197, 1024]       1,048,576\n",
      "           Linear-82            [-1, 197, 1024]       1,048,576\n",
      "           Linear-83            [-1, 197, 1024]       1,048,576\n",
      "           Linear-84            [-1, 197, 1024]       1,049,600\n",
      "     MultiHeadAtt-85            [-1, 197, 1024]               0\n",
      "        LayerNorm-86            [-1, 197, 1024]           2,048\n",
      "           Linear-87            [-1, 197, 4096]       4,198,400\n",
      "           Linear-88            [-1, 197, 1024]       4,195,328\n",
      "              FFN-89            [-1, 197, 1024]               0\n",
      "TransformerEncoder-90            [-1, 197, 1024]               0\n",
      "        LayerNorm-91            [-1, 197, 1024]           2,048\n",
      "           Linear-92            [-1, 197, 1024]       1,048,576\n",
      "           Linear-93            [-1, 197, 1024]       1,048,576\n",
      "           Linear-94            [-1, 197, 1024]       1,048,576\n",
      "           Linear-95            [-1, 197, 1024]       1,049,600\n",
      "     MultiHeadAtt-96            [-1, 197, 1024]               0\n",
      "        LayerNorm-97            [-1, 197, 1024]           2,048\n",
      "           Linear-98            [-1, 197, 4096]       4,198,400\n",
      "           Linear-99            [-1, 197, 1024]       4,195,328\n",
      "             FFN-100            [-1, 197, 1024]               0\n",
      "TransformerEncoder-101            [-1, 197, 1024]               0\n",
      "       LayerNorm-102            [-1, 197, 1024]           2,048\n",
      "          Linear-103            [-1, 197, 1024]       1,048,576\n",
      "          Linear-104            [-1, 197, 1024]       1,048,576\n",
      "          Linear-105            [-1, 197, 1024]       1,048,576\n",
      "          Linear-106            [-1, 197, 1024]       1,049,600\n",
      "    MultiHeadAtt-107            [-1, 197, 1024]               0\n",
      "       LayerNorm-108            [-1, 197, 1024]           2,048\n",
      "          Linear-109            [-1, 197, 4096]       4,198,400\n",
      "          Linear-110            [-1, 197, 1024]       4,195,328\n",
      "             FFN-111            [-1, 197, 1024]               0\n",
      "TransformerEncoder-112            [-1, 197, 1024]               0\n",
      "       LayerNorm-113            [-1, 197, 1024]           2,048\n",
      "          Linear-114            [-1, 197, 1024]       1,048,576\n",
      "          Linear-115            [-1, 197, 1024]       1,048,576\n",
      "          Linear-116            [-1, 197, 1024]       1,048,576\n",
      "          Linear-117            [-1, 197, 1024]       1,049,600\n",
      "    MultiHeadAtt-118            [-1, 197, 1024]               0\n",
      "       LayerNorm-119            [-1, 197, 1024]           2,048\n",
      "          Linear-120            [-1, 197, 4096]       4,198,400\n",
      "          Linear-121            [-1, 197, 1024]       4,195,328\n",
      "             FFN-122            [-1, 197, 1024]               0\n",
      "TransformerEncoder-123            [-1, 197, 1024]               0\n",
      "       LayerNorm-124            [-1, 197, 1024]           2,048\n",
      "          Linear-125            [-1, 197, 1024]       1,048,576\n",
      "          Linear-126            [-1, 197, 1024]       1,048,576\n",
      "          Linear-127            [-1, 197, 1024]       1,048,576\n",
      "          Linear-128            [-1, 197, 1024]       1,049,600\n",
      "    MultiHeadAtt-129            [-1, 197, 1024]               0\n",
      "       LayerNorm-130            [-1, 197, 1024]           2,048\n",
      "          Linear-131            [-1, 197, 4096]       4,198,400\n",
      "          Linear-132            [-1, 197, 1024]       4,195,328\n",
      "             FFN-133            [-1, 197, 1024]               0\n",
      "TransformerEncoder-134            [-1, 197, 1024]               0\n",
      "       LayerNorm-135            [-1, 197, 1024]           2,048\n",
      "          Linear-136            [-1, 197, 1024]       1,048,576\n",
      "          Linear-137            [-1, 197, 1024]       1,048,576\n",
      "          Linear-138            [-1, 197, 1024]       1,048,576\n",
      "          Linear-139            [-1, 197, 1024]       1,049,600\n",
      "    MultiHeadAtt-140            [-1, 197, 1024]               0\n",
      "       LayerNorm-141            [-1, 197, 1024]           2,048\n",
      "          Linear-142            [-1, 197, 4096]       4,198,400\n",
      "          Linear-143            [-1, 197, 1024]       4,195,328\n",
      "             FFN-144            [-1, 197, 1024]               0\n",
      "TransformerEncoder-145            [-1, 197, 1024]               0\n",
      "       LayerNorm-146            [-1, 197, 1024]           2,048\n",
      "          Linear-147            [-1, 197, 1024]       1,048,576\n",
      "          Linear-148            [-1, 197, 1024]       1,048,576\n",
      "          Linear-149            [-1, 197, 1024]       1,048,576\n",
      "          Linear-150            [-1, 197, 1024]       1,049,600\n",
      "    MultiHeadAtt-151            [-1, 197, 1024]               0\n",
      "       LayerNorm-152            [-1, 197, 1024]           2,048\n",
      "          Linear-153            [-1, 197, 4096]       4,198,400\n",
      "          Linear-154            [-1, 197, 1024]       4,195,328\n",
      "             FFN-155            [-1, 197, 1024]               0\n",
      "TransformerEncoder-156            [-1, 197, 1024]               0\n",
      "       LayerNorm-157            [-1, 197, 1024]           2,048\n",
      "          Linear-158            [-1, 197, 1024]       1,048,576\n",
      "          Linear-159            [-1, 197, 1024]       1,048,576\n",
      "          Linear-160            [-1, 197, 1024]       1,048,576\n",
      "          Linear-161            [-1, 197, 1024]       1,049,600\n",
      "    MultiHeadAtt-162            [-1, 197, 1024]               0\n",
      "       LayerNorm-163            [-1, 197, 1024]           2,048\n",
      "          Linear-164            [-1, 197, 4096]       4,198,400\n",
      "          Linear-165            [-1, 197, 1024]       4,195,328\n",
      "             FFN-166            [-1, 197, 1024]               0\n",
      "TransformerEncoder-167            [-1, 197, 1024]               0\n",
      "       LayerNorm-168            [-1, 197, 1024]           2,048\n",
      "          Linear-169            [-1, 197, 1024]       1,048,576\n",
      "          Linear-170            [-1, 197, 1024]       1,048,576\n",
      "          Linear-171            [-1, 197, 1024]       1,048,576\n",
      "          Linear-172            [-1, 197, 1024]       1,049,600\n",
      "    MultiHeadAtt-173            [-1, 197, 1024]               0\n",
      "       LayerNorm-174            [-1, 197, 1024]           2,048\n",
      "          Linear-175            [-1, 197, 4096]       4,198,400\n",
      "          Linear-176            [-1, 197, 1024]       4,195,328\n",
      "             FFN-177            [-1, 197, 1024]               0\n",
      "TransformerEncoder-178            [-1, 197, 1024]               0\n",
      "       LayerNorm-179            [-1, 197, 1024]           2,048\n",
      "          Linear-180            [-1, 197, 1024]       1,048,576\n",
      "          Linear-181            [-1, 197, 1024]       1,048,576\n",
      "          Linear-182            [-1, 197, 1024]       1,048,576\n",
      "          Linear-183            [-1, 197, 1024]       1,049,600\n",
      "    MultiHeadAtt-184            [-1, 197, 1024]               0\n",
      "       LayerNorm-185            [-1, 197, 1024]           2,048\n",
      "          Linear-186            [-1, 197, 4096]       4,198,400\n",
      "          Linear-187            [-1, 197, 1024]       4,195,328\n",
      "             FFN-188            [-1, 197, 1024]               0\n",
      "TransformerEncoder-189            [-1, 197, 1024]               0\n",
      "       LayerNorm-190            [-1, 197, 1024]           2,048\n",
      "          Linear-191            [-1, 197, 1024]       1,048,576\n",
      "          Linear-192            [-1, 197, 1024]       1,048,576\n",
      "          Linear-193            [-1, 197, 1024]       1,048,576\n",
      "          Linear-194            [-1, 197, 1024]       1,049,600\n",
      "    MultiHeadAtt-195            [-1, 197, 1024]               0\n",
      "       LayerNorm-196            [-1, 197, 1024]           2,048\n",
      "          Linear-197            [-1, 197, 4096]       4,198,400\n",
      "          Linear-198            [-1, 197, 1024]       4,195,328\n",
      "             FFN-199            [-1, 197, 1024]               0\n",
      "TransformerEncoder-200            [-1, 197, 1024]               0\n",
      "       LayerNorm-201            [-1, 197, 1024]           2,048\n",
      "          Linear-202            [-1, 197, 1024]       1,048,576\n",
      "          Linear-203            [-1, 197, 1024]       1,048,576\n",
      "          Linear-204            [-1, 197, 1024]       1,048,576\n",
      "          Linear-205            [-1, 197, 1024]       1,049,600\n",
      "    MultiHeadAtt-206            [-1, 197, 1024]               0\n",
      "       LayerNorm-207            [-1, 197, 1024]           2,048\n",
      "          Linear-208            [-1, 197, 4096]       4,198,400\n",
      "          Linear-209            [-1, 197, 1024]       4,195,328\n",
      "             FFN-210            [-1, 197, 1024]               0\n",
      "TransformerEncoder-211            [-1, 197, 1024]               0\n",
      "       LayerNorm-212            [-1, 197, 1024]           2,048\n",
      "          Linear-213            [-1, 197, 1024]       1,048,576\n",
      "          Linear-214            [-1, 197, 1024]       1,048,576\n",
      "          Linear-215            [-1, 197, 1024]       1,048,576\n",
      "          Linear-216            [-1, 197, 1024]       1,049,600\n",
      "    MultiHeadAtt-217            [-1, 197, 1024]               0\n",
      "       LayerNorm-218            [-1, 197, 1024]           2,048\n",
      "          Linear-219            [-1, 197, 4096]       4,198,400\n",
      "          Linear-220            [-1, 197, 1024]       4,195,328\n",
      "             FFN-221            [-1, 197, 1024]               0\n",
      "TransformerEncoder-222            [-1, 197, 1024]               0\n",
      "       LayerNorm-223            [-1, 197, 1024]           2,048\n",
      "          Linear-224            [-1, 197, 1024]       1,048,576\n",
      "          Linear-225            [-1, 197, 1024]       1,048,576\n",
      "          Linear-226            [-1, 197, 1024]       1,048,576\n",
      "          Linear-227            [-1, 197, 1024]       1,049,600\n",
      "    MultiHeadAtt-228            [-1, 197, 1024]               0\n",
      "       LayerNorm-229            [-1, 197, 1024]           2,048\n",
      "          Linear-230            [-1, 197, 4096]       4,198,400\n",
      "          Linear-231            [-1, 197, 1024]       4,195,328\n",
      "             FFN-232            [-1, 197, 1024]               0\n",
      "TransformerEncoder-233            [-1, 197, 1024]               0\n",
      "       LayerNorm-234            [-1, 197, 1024]           2,048\n",
      "          Linear-235            [-1, 197, 1024]       1,048,576\n",
      "          Linear-236            [-1, 197, 1024]       1,048,576\n",
      "          Linear-237            [-1, 197, 1024]       1,048,576\n",
      "          Linear-238            [-1, 197, 1024]       1,049,600\n",
      "    MultiHeadAtt-239            [-1, 197, 1024]               0\n",
      "       LayerNorm-240            [-1, 197, 1024]           2,048\n",
      "          Linear-241            [-1, 197, 4096]       4,198,400\n",
      "          Linear-242            [-1, 197, 1024]       4,195,328\n",
      "             FFN-243            [-1, 197, 1024]               0\n",
      "TransformerEncoder-244            [-1, 197, 1024]               0\n",
      "       LayerNorm-245            [-1, 197, 1024]           2,048\n",
      "          Linear-246            [-1, 197, 1024]       1,048,576\n",
      "          Linear-247            [-1, 197, 1024]       1,048,576\n",
      "          Linear-248            [-1, 197, 1024]       1,048,576\n",
      "          Linear-249            [-1, 197, 1024]       1,049,600\n",
      "    MultiHeadAtt-250            [-1, 197, 1024]               0\n",
      "       LayerNorm-251            [-1, 197, 1024]           2,048\n",
      "          Linear-252            [-1, 197, 4096]       4,198,400\n",
      "          Linear-253            [-1, 197, 1024]       4,195,328\n",
      "             FFN-254            [-1, 197, 1024]               0\n",
      "TransformerEncoder-255            [-1, 197, 1024]               0\n",
      "       LayerNorm-256            [-1, 197, 1024]           2,048\n",
      "          Linear-257            [-1, 197, 1024]       1,048,576\n",
      "          Linear-258            [-1, 197, 1024]       1,048,576\n",
      "          Linear-259            [-1, 197, 1024]       1,048,576\n",
      "          Linear-260            [-1, 197, 1024]       1,049,600\n",
      "    MultiHeadAtt-261            [-1, 197, 1024]               0\n",
      "       LayerNorm-262            [-1, 197, 1024]           2,048\n",
      "          Linear-263            [-1, 197, 4096]       4,198,400\n",
      "          Linear-264            [-1, 197, 1024]       4,195,328\n",
      "             FFN-265            [-1, 197, 1024]               0\n",
      "TransformerEncoder-266            [-1, 197, 1024]               0\n",
      "          Linear-267                   [-1, 10]          10,250\n",
      "       LayerNorm-268                   [-1, 10]              20\n",
      "================================================================\n",
      "Total params: 303,033,374\n",
      "Trainable params: 303,033,374\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 520.19\n",
      "Params size (MB): 1155.98\n",
      "Estimated Total Size (MB): 1676.74\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ViT variants -- ViT-L/16\n",
    "n_class = 10\n",
    "n_layer = 24\n",
    "d_model = 1024\n",
    "n_head = 16\n",
    "mlp_hidden = 4096\n",
    "\n",
    "vit_encoder = nn.Sequential(*[TransformerEncoder(d_model=d_model, head=n_head, d_ff_hid=mlp_hidden) for i in range(n_layer)])\n",
    "vit_l = ViT(img_size=224, patch_size=16, img_c=3, d_model=d_model, num_class=n_class, encoders=vit_encoder)\n",
    "\n",
    "input_tensor = torch.ones((1, 3, 224, 224))\n",
    "\n",
    "output_tensor = vit_l(input_tensor)\n",
    "assert output_tensor.shape == (1, n_class), output_tensor.shape\n",
    "summary(vit_l, input_size=(3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0238ed10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 1280, 14, 14]         984,320\n",
      "        PatchEmbed-2            [-1, 196, 1280]               0\n",
      "         LayerNorm-3            [-1, 197, 1280]           2,560\n",
      "            Linear-4            [-1, 197, 1280]       1,638,400\n",
      "            Linear-5            [-1, 197, 1280]       1,638,400\n",
      "            Linear-6            [-1, 197, 1280]       1,638,400\n",
      "            Linear-7            [-1, 197, 1280]       1,639,680\n",
      "      MultiHeadAtt-8            [-1, 197, 1280]               0\n",
      "         LayerNorm-9            [-1, 197, 1280]           2,560\n",
      "           Linear-10            [-1, 197, 5120]       6,558,720\n",
      "           Linear-11            [-1, 197, 1280]       6,554,880\n",
      "              FFN-12            [-1, 197, 1280]               0\n",
      "TransformerEncoder-13            [-1, 197, 1280]               0\n",
      "        LayerNorm-14            [-1, 197, 1280]           2,560\n",
      "           Linear-15            [-1, 197, 1280]       1,638,400\n",
      "           Linear-16            [-1, 197, 1280]       1,638,400\n",
      "           Linear-17            [-1, 197, 1280]       1,638,400\n",
      "           Linear-18            [-1, 197, 1280]       1,639,680\n",
      "     MultiHeadAtt-19            [-1, 197, 1280]               0\n",
      "        LayerNorm-20            [-1, 197, 1280]           2,560\n",
      "           Linear-21            [-1, 197, 5120]       6,558,720\n",
      "           Linear-22            [-1, 197, 1280]       6,554,880\n",
      "              FFN-23            [-1, 197, 1280]               0\n",
      "TransformerEncoder-24            [-1, 197, 1280]               0\n",
      "        LayerNorm-25            [-1, 197, 1280]           2,560\n",
      "           Linear-26            [-1, 197, 1280]       1,638,400\n",
      "           Linear-27            [-1, 197, 1280]       1,638,400\n",
      "           Linear-28            [-1, 197, 1280]       1,638,400\n",
      "           Linear-29            [-1, 197, 1280]       1,639,680\n",
      "     MultiHeadAtt-30            [-1, 197, 1280]               0\n",
      "        LayerNorm-31            [-1, 197, 1280]           2,560\n",
      "           Linear-32            [-1, 197, 5120]       6,558,720\n",
      "           Linear-33            [-1, 197, 1280]       6,554,880\n",
      "              FFN-34            [-1, 197, 1280]               0\n",
      "TransformerEncoder-35            [-1, 197, 1280]               0\n",
      "        LayerNorm-36            [-1, 197, 1280]           2,560\n",
      "           Linear-37            [-1, 197, 1280]       1,638,400\n",
      "           Linear-38            [-1, 197, 1280]       1,638,400\n",
      "           Linear-39            [-1, 197, 1280]       1,638,400\n",
      "           Linear-40            [-1, 197, 1280]       1,639,680\n",
      "     MultiHeadAtt-41            [-1, 197, 1280]               0\n",
      "        LayerNorm-42            [-1, 197, 1280]           2,560\n",
      "           Linear-43            [-1, 197, 5120]       6,558,720\n",
      "           Linear-44            [-1, 197, 1280]       6,554,880\n",
      "              FFN-45            [-1, 197, 1280]               0\n",
      "TransformerEncoder-46            [-1, 197, 1280]               0\n",
      "        LayerNorm-47            [-1, 197, 1280]           2,560\n",
      "           Linear-48            [-1, 197, 1280]       1,638,400\n",
      "           Linear-49            [-1, 197, 1280]       1,638,400\n",
      "           Linear-50            [-1, 197, 1280]       1,638,400\n",
      "           Linear-51            [-1, 197, 1280]       1,639,680\n",
      "     MultiHeadAtt-52            [-1, 197, 1280]               0\n",
      "        LayerNorm-53            [-1, 197, 1280]           2,560\n",
      "           Linear-54            [-1, 197, 5120]       6,558,720\n",
      "           Linear-55            [-1, 197, 1280]       6,554,880\n",
      "              FFN-56            [-1, 197, 1280]               0\n",
      "TransformerEncoder-57            [-1, 197, 1280]               0\n",
      "        LayerNorm-58            [-1, 197, 1280]           2,560\n",
      "           Linear-59            [-1, 197, 1280]       1,638,400\n",
      "           Linear-60            [-1, 197, 1280]       1,638,400\n",
      "           Linear-61            [-1, 197, 1280]       1,638,400\n",
      "           Linear-62            [-1, 197, 1280]       1,639,680\n",
      "     MultiHeadAtt-63            [-1, 197, 1280]               0\n",
      "        LayerNorm-64            [-1, 197, 1280]           2,560\n",
      "           Linear-65            [-1, 197, 5120]       6,558,720\n",
      "           Linear-66            [-1, 197, 1280]       6,554,880\n",
      "              FFN-67            [-1, 197, 1280]               0\n",
      "TransformerEncoder-68            [-1, 197, 1280]               0\n",
      "        LayerNorm-69            [-1, 197, 1280]           2,560\n",
      "           Linear-70            [-1, 197, 1280]       1,638,400\n",
      "           Linear-71            [-1, 197, 1280]       1,638,400\n",
      "           Linear-72            [-1, 197, 1280]       1,638,400\n",
      "           Linear-73            [-1, 197, 1280]       1,639,680\n",
      "     MultiHeadAtt-74            [-1, 197, 1280]               0\n",
      "        LayerNorm-75            [-1, 197, 1280]           2,560\n",
      "           Linear-76            [-1, 197, 5120]       6,558,720\n",
      "           Linear-77            [-1, 197, 1280]       6,554,880\n",
      "              FFN-78            [-1, 197, 1280]               0\n",
      "TransformerEncoder-79            [-1, 197, 1280]               0\n",
      "        LayerNorm-80            [-1, 197, 1280]           2,560\n",
      "           Linear-81            [-1, 197, 1280]       1,638,400\n",
      "           Linear-82            [-1, 197, 1280]       1,638,400\n",
      "           Linear-83            [-1, 197, 1280]       1,638,400\n",
      "           Linear-84            [-1, 197, 1280]       1,639,680\n",
      "     MultiHeadAtt-85            [-1, 197, 1280]               0\n",
      "        LayerNorm-86            [-1, 197, 1280]           2,560\n",
      "           Linear-87            [-1, 197, 5120]       6,558,720\n",
      "           Linear-88            [-1, 197, 1280]       6,554,880\n",
      "              FFN-89            [-1, 197, 1280]               0\n",
      "TransformerEncoder-90            [-1, 197, 1280]               0\n",
      "        LayerNorm-91            [-1, 197, 1280]           2,560\n",
      "           Linear-92            [-1, 197, 1280]       1,638,400\n",
      "           Linear-93            [-1, 197, 1280]       1,638,400\n",
      "           Linear-94            [-1, 197, 1280]       1,638,400\n",
      "           Linear-95            [-1, 197, 1280]       1,639,680\n",
      "     MultiHeadAtt-96            [-1, 197, 1280]               0\n",
      "        LayerNorm-97            [-1, 197, 1280]           2,560\n",
      "           Linear-98            [-1, 197, 5120]       6,558,720\n",
      "           Linear-99            [-1, 197, 1280]       6,554,880\n",
      "             FFN-100            [-1, 197, 1280]               0\n",
      "TransformerEncoder-101            [-1, 197, 1280]               0\n",
      "       LayerNorm-102            [-1, 197, 1280]           2,560\n",
      "          Linear-103            [-1, 197, 1280]       1,638,400\n",
      "          Linear-104            [-1, 197, 1280]       1,638,400\n",
      "          Linear-105            [-1, 197, 1280]       1,638,400\n",
      "          Linear-106            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-107            [-1, 197, 1280]               0\n",
      "       LayerNorm-108            [-1, 197, 1280]           2,560\n",
      "          Linear-109            [-1, 197, 5120]       6,558,720\n",
      "          Linear-110            [-1, 197, 1280]       6,554,880\n",
      "             FFN-111            [-1, 197, 1280]               0\n",
      "TransformerEncoder-112            [-1, 197, 1280]               0\n",
      "       LayerNorm-113            [-1, 197, 1280]           2,560\n",
      "          Linear-114            [-1, 197, 1280]       1,638,400\n",
      "          Linear-115            [-1, 197, 1280]       1,638,400\n",
      "          Linear-116            [-1, 197, 1280]       1,638,400\n",
      "          Linear-117            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-118            [-1, 197, 1280]               0\n",
      "       LayerNorm-119            [-1, 197, 1280]           2,560\n",
      "          Linear-120            [-1, 197, 5120]       6,558,720\n",
      "          Linear-121            [-1, 197, 1280]       6,554,880\n",
      "             FFN-122            [-1, 197, 1280]               0\n",
      "TransformerEncoder-123            [-1, 197, 1280]               0\n",
      "       LayerNorm-124            [-1, 197, 1280]           2,560\n",
      "          Linear-125            [-1, 197, 1280]       1,638,400\n",
      "          Linear-126            [-1, 197, 1280]       1,638,400\n",
      "          Linear-127            [-1, 197, 1280]       1,638,400\n",
      "          Linear-128            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-129            [-1, 197, 1280]               0\n",
      "       LayerNorm-130            [-1, 197, 1280]           2,560\n",
      "          Linear-131            [-1, 197, 5120]       6,558,720\n",
      "          Linear-132            [-1, 197, 1280]       6,554,880\n",
      "             FFN-133            [-1, 197, 1280]               0\n",
      "TransformerEncoder-134            [-1, 197, 1280]               0\n",
      "       LayerNorm-135            [-1, 197, 1280]           2,560\n",
      "          Linear-136            [-1, 197, 1280]       1,638,400\n",
      "          Linear-137            [-1, 197, 1280]       1,638,400\n",
      "          Linear-138            [-1, 197, 1280]       1,638,400\n",
      "          Linear-139            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-140            [-1, 197, 1280]               0\n",
      "       LayerNorm-141            [-1, 197, 1280]           2,560\n",
      "          Linear-142            [-1, 197, 5120]       6,558,720\n",
      "          Linear-143            [-1, 197, 1280]       6,554,880\n",
      "             FFN-144            [-1, 197, 1280]               0\n",
      "TransformerEncoder-145            [-1, 197, 1280]               0\n",
      "       LayerNorm-146            [-1, 197, 1280]           2,560\n",
      "          Linear-147            [-1, 197, 1280]       1,638,400\n",
      "          Linear-148            [-1, 197, 1280]       1,638,400\n",
      "          Linear-149            [-1, 197, 1280]       1,638,400\n",
      "          Linear-150            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-151            [-1, 197, 1280]               0\n",
      "       LayerNorm-152            [-1, 197, 1280]           2,560\n",
      "          Linear-153            [-1, 197, 5120]       6,558,720\n",
      "          Linear-154            [-1, 197, 1280]       6,554,880\n",
      "             FFN-155            [-1, 197, 1280]               0\n",
      "TransformerEncoder-156            [-1, 197, 1280]               0\n",
      "       LayerNorm-157            [-1, 197, 1280]           2,560\n",
      "          Linear-158            [-1, 197, 1280]       1,638,400\n",
      "          Linear-159            [-1, 197, 1280]       1,638,400\n",
      "          Linear-160            [-1, 197, 1280]       1,638,400\n",
      "          Linear-161            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-162            [-1, 197, 1280]               0\n",
      "       LayerNorm-163            [-1, 197, 1280]           2,560\n",
      "          Linear-164            [-1, 197, 5120]       6,558,720\n",
      "          Linear-165            [-1, 197, 1280]       6,554,880\n",
      "             FFN-166            [-1, 197, 1280]               0\n",
      "TransformerEncoder-167            [-1, 197, 1280]               0\n",
      "       LayerNorm-168            [-1, 197, 1280]           2,560\n",
      "          Linear-169            [-1, 197, 1280]       1,638,400\n",
      "          Linear-170            [-1, 197, 1280]       1,638,400\n",
      "          Linear-171            [-1, 197, 1280]       1,638,400\n",
      "          Linear-172            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-173            [-1, 197, 1280]               0\n",
      "       LayerNorm-174            [-1, 197, 1280]           2,560\n",
      "          Linear-175            [-1, 197, 5120]       6,558,720\n",
      "          Linear-176            [-1, 197, 1280]       6,554,880\n",
      "             FFN-177            [-1, 197, 1280]               0\n",
      "TransformerEncoder-178            [-1, 197, 1280]               0\n",
      "       LayerNorm-179            [-1, 197, 1280]           2,560\n",
      "          Linear-180            [-1, 197, 1280]       1,638,400\n",
      "          Linear-181            [-1, 197, 1280]       1,638,400\n",
      "          Linear-182            [-1, 197, 1280]       1,638,400\n",
      "          Linear-183            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-184            [-1, 197, 1280]               0\n",
      "       LayerNorm-185            [-1, 197, 1280]           2,560\n",
      "          Linear-186            [-1, 197, 5120]       6,558,720\n",
      "          Linear-187            [-1, 197, 1280]       6,554,880\n",
      "             FFN-188            [-1, 197, 1280]               0\n",
      "TransformerEncoder-189            [-1, 197, 1280]               0\n",
      "       LayerNorm-190            [-1, 197, 1280]           2,560\n",
      "          Linear-191            [-1, 197, 1280]       1,638,400\n",
      "          Linear-192            [-1, 197, 1280]       1,638,400\n",
      "          Linear-193            [-1, 197, 1280]       1,638,400\n",
      "          Linear-194            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-195            [-1, 197, 1280]               0\n",
      "       LayerNorm-196            [-1, 197, 1280]           2,560\n",
      "          Linear-197            [-1, 197, 5120]       6,558,720\n",
      "          Linear-198            [-1, 197, 1280]       6,554,880\n",
      "             FFN-199            [-1, 197, 1280]               0\n",
      "TransformerEncoder-200            [-1, 197, 1280]               0\n",
      "       LayerNorm-201            [-1, 197, 1280]           2,560\n",
      "          Linear-202            [-1, 197, 1280]       1,638,400\n",
      "          Linear-203            [-1, 197, 1280]       1,638,400\n",
      "          Linear-204            [-1, 197, 1280]       1,638,400\n",
      "          Linear-205            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-206            [-1, 197, 1280]               0\n",
      "       LayerNorm-207            [-1, 197, 1280]           2,560\n",
      "          Linear-208            [-1, 197, 5120]       6,558,720\n",
      "          Linear-209            [-1, 197, 1280]       6,554,880\n",
      "             FFN-210            [-1, 197, 1280]               0\n",
      "TransformerEncoder-211            [-1, 197, 1280]               0\n",
      "       LayerNorm-212            [-1, 197, 1280]           2,560\n",
      "          Linear-213            [-1, 197, 1280]       1,638,400\n",
      "          Linear-214            [-1, 197, 1280]       1,638,400\n",
      "          Linear-215            [-1, 197, 1280]       1,638,400\n",
      "          Linear-216            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-217            [-1, 197, 1280]               0\n",
      "       LayerNorm-218            [-1, 197, 1280]           2,560\n",
      "          Linear-219            [-1, 197, 5120]       6,558,720\n",
      "          Linear-220            [-1, 197, 1280]       6,554,880\n",
      "             FFN-221            [-1, 197, 1280]               0\n",
      "TransformerEncoder-222            [-1, 197, 1280]               0\n",
      "       LayerNorm-223            [-1, 197, 1280]           2,560\n",
      "          Linear-224            [-1, 197, 1280]       1,638,400\n",
      "          Linear-225            [-1, 197, 1280]       1,638,400\n",
      "          Linear-226            [-1, 197, 1280]       1,638,400\n",
      "          Linear-227            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-228            [-1, 197, 1280]               0\n",
      "       LayerNorm-229            [-1, 197, 1280]           2,560\n",
      "          Linear-230            [-1, 197, 5120]       6,558,720\n",
      "          Linear-231            [-1, 197, 1280]       6,554,880\n",
      "             FFN-232            [-1, 197, 1280]               0\n",
      "TransformerEncoder-233            [-1, 197, 1280]               0\n",
      "       LayerNorm-234            [-1, 197, 1280]           2,560\n",
      "          Linear-235            [-1, 197, 1280]       1,638,400\n",
      "          Linear-236            [-1, 197, 1280]       1,638,400\n",
      "          Linear-237            [-1, 197, 1280]       1,638,400\n",
      "          Linear-238            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-239            [-1, 197, 1280]               0\n",
      "       LayerNorm-240            [-1, 197, 1280]           2,560\n",
      "          Linear-241            [-1, 197, 5120]       6,558,720\n",
      "          Linear-242            [-1, 197, 1280]       6,554,880\n",
      "             FFN-243            [-1, 197, 1280]               0\n",
      "TransformerEncoder-244            [-1, 197, 1280]               0\n",
      "       LayerNorm-245            [-1, 197, 1280]           2,560\n",
      "          Linear-246            [-1, 197, 1280]       1,638,400\n",
      "          Linear-247            [-1, 197, 1280]       1,638,400\n",
      "          Linear-248            [-1, 197, 1280]       1,638,400\n",
      "          Linear-249            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-250            [-1, 197, 1280]               0\n",
      "       LayerNorm-251            [-1, 197, 1280]           2,560\n",
      "          Linear-252            [-1, 197, 5120]       6,558,720\n",
      "          Linear-253            [-1, 197, 1280]       6,554,880\n",
      "             FFN-254            [-1, 197, 1280]               0\n",
      "TransformerEncoder-255            [-1, 197, 1280]               0\n",
      "       LayerNorm-256            [-1, 197, 1280]           2,560\n",
      "          Linear-257            [-1, 197, 1280]       1,638,400\n",
      "          Linear-258            [-1, 197, 1280]       1,638,400\n",
      "          Linear-259            [-1, 197, 1280]       1,638,400\n",
      "          Linear-260            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-261            [-1, 197, 1280]               0\n",
      "       LayerNorm-262            [-1, 197, 1280]           2,560\n",
      "          Linear-263            [-1, 197, 5120]       6,558,720\n",
      "          Linear-264            [-1, 197, 1280]       6,554,880\n",
      "             FFN-265            [-1, 197, 1280]               0\n",
      "TransformerEncoder-266            [-1, 197, 1280]               0\n",
      "       LayerNorm-267            [-1, 197, 1280]           2,560\n",
      "          Linear-268            [-1, 197, 1280]       1,638,400\n",
      "          Linear-269            [-1, 197, 1280]       1,638,400\n",
      "          Linear-270            [-1, 197, 1280]       1,638,400\n",
      "          Linear-271            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-272            [-1, 197, 1280]               0\n",
      "       LayerNorm-273            [-1, 197, 1280]           2,560\n",
      "          Linear-274            [-1, 197, 5120]       6,558,720\n",
      "          Linear-275            [-1, 197, 1280]       6,554,880\n",
      "             FFN-276            [-1, 197, 1280]               0\n",
      "TransformerEncoder-277            [-1, 197, 1280]               0\n",
      "       LayerNorm-278            [-1, 197, 1280]           2,560\n",
      "          Linear-279            [-1, 197, 1280]       1,638,400\n",
      "          Linear-280            [-1, 197, 1280]       1,638,400\n",
      "          Linear-281            [-1, 197, 1280]       1,638,400\n",
      "          Linear-282            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-283            [-1, 197, 1280]               0\n",
      "       LayerNorm-284            [-1, 197, 1280]           2,560\n",
      "          Linear-285            [-1, 197, 5120]       6,558,720\n",
      "          Linear-286            [-1, 197, 1280]       6,554,880\n",
      "             FFN-287            [-1, 197, 1280]               0\n",
      "TransformerEncoder-288            [-1, 197, 1280]               0\n",
      "       LayerNorm-289            [-1, 197, 1280]           2,560\n",
      "          Linear-290            [-1, 197, 1280]       1,638,400\n",
      "          Linear-291            [-1, 197, 1280]       1,638,400\n",
      "          Linear-292            [-1, 197, 1280]       1,638,400\n",
      "          Linear-293            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-294            [-1, 197, 1280]               0\n",
      "       LayerNorm-295            [-1, 197, 1280]           2,560\n",
      "          Linear-296            [-1, 197, 5120]       6,558,720\n",
      "          Linear-297            [-1, 197, 1280]       6,554,880\n",
      "             FFN-298            [-1, 197, 1280]               0\n",
      "TransformerEncoder-299            [-1, 197, 1280]               0\n",
      "       LayerNorm-300            [-1, 197, 1280]           2,560\n",
      "          Linear-301            [-1, 197, 1280]       1,638,400\n",
      "          Linear-302            [-1, 197, 1280]       1,638,400\n",
      "          Linear-303            [-1, 197, 1280]       1,638,400\n",
      "          Linear-304            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-305            [-1, 197, 1280]               0\n",
      "       LayerNorm-306            [-1, 197, 1280]           2,560\n",
      "          Linear-307            [-1, 197, 5120]       6,558,720\n",
      "          Linear-308            [-1, 197, 1280]       6,554,880\n",
      "             FFN-309            [-1, 197, 1280]               0\n",
      "TransformerEncoder-310            [-1, 197, 1280]               0\n",
      "       LayerNorm-311            [-1, 197, 1280]           2,560\n",
      "          Linear-312            [-1, 197, 1280]       1,638,400\n",
      "          Linear-313            [-1, 197, 1280]       1,638,400\n",
      "          Linear-314            [-1, 197, 1280]       1,638,400\n",
      "          Linear-315            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-316            [-1, 197, 1280]               0\n",
      "       LayerNorm-317            [-1, 197, 1280]           2,560\n",
      "          Linear-318            [-1, 197, 5120]       6,558,720\n",
      "          Linear-319            [-1, 197, 1280]       6,554,880\n",
      "             FFN-320            [-1, 197, 1280]               0\n",
      "TransformerEncoder-321            [-1, 197, 1280]               0\n",
      "       LayerNorm-322            [-1, 197, 1280]           2,560\n",
      "          Linear-323            [-1, 197, 1280]       1,638,400\n",
      "          Linear-324            [-1, 197, 1280]       1,638,400\n",
      "          Linear-325            [-1, 197, 1280]       1,638,400\n",
      "          Linear-326            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-327            [-1, 197, 1280]               0\n",
      "       LayerNorm-328            [-1, 197, 1280]           2,560\n",
      "          Linear-329            [-1, 197, 5120]       6,558,720\n",
      "          Linear-330            [-1, 197, 1280]       6,554,880\n",
      "             FFN-331            [-1, 197, 1280]               0\n",
      "TransformerEncoder-332            [-1, 197, 1280]               0\n",
      "       LayerNorm-333            [-1, 197, 1280]           2,560\n",
      "          Linear-334            [-1, 197, 1280]       1,638,400\n",
      "          Linear-335            [-1, 197, 1280]       1,638,400\n",
      "          Linear-336            [-1, 197, 1280]       1,638,400\n",
      "          Linear-337            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-338            [-1, 197, 1280]               0\n",
      "       LayerNorm-339            [-1, 197, 1280]           2,560\n",
      "          Linear-340            [-1, 197, 5120]       6,558,720\n",
      "          Linear-341            [-1, 197, 1280]       6,554,880\n",
      "             FFN-342            [-1, 197, 1280]               0\n",
      "TransformerEncoder-343            [-1, 197, 1280]               0\n",
      "       LayerNorm-344            [-1, 197, 1280]           2,560\n",
      "          Linear-345            [-1, 197, 1280]       1,638,400\n",
      "          Linear-346            [-1, 197, 1280]       1,638,400\n",
      "          Linear-347            [-1, 197, 1280]       1,638,400\n",
      "          Linear-348            [-1, 197, 1280]       1,639,680\n",
      "    MultiHeadAtt-349            [-1, 197, 1280]               0\n",
      "       LayerNorm-350            [-1, 197, 1280]           2,560\n",
      "          Linear-351            [-1, 197, 5120]       6,558,720\n",
      "          Linear-352            [-1, 197, 1280]       6,554,880\n",
      "             FFN-353            [-1, 197, 1280]               0\n",
      "TransformerEncoder-354            [-1, 197, 1280]               0\n",
      "          Linear-355                   [-1, 10]          12,810\n",
      "       LayerNorm-356                   [-1, 10]              20\n",
      "================================================================\n",
      "Total params: 630,552,350\n",
      "Trainable params: 630,552,350\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 865.70\n",
      "Params size (MB): 2405.37\n",
      "Estimated Total Size (MB): 3271.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ViT variants -- ViT-H/16\n",
    "n_class = 10\n",
    "n_layer = 32\n",
    "d_model = 1280\n",
    "n_head = 16\n",
    "mlp_hidden = 5120\n",
    "\n",
    "vit_encoder = nn.Sequential(*[TransformerEncoder(d_model=d_model, head=n_head, d_ff_hid=mlp_hidden) for i in range(n_layer)])\n",
    "vit_h = ViT(img_size=224, patch_size=16, img_c=3, d_model=d_model, num_class=n_class, encoders=vit_encoder)\n",
    "\n",
    "input_tensor = torch.ones((1, 3, 224, 224))\n",
    "\n",
    "output_tensor = vit_h(input_tensor)\n",
    "assert output_tensor.shape == (1, n_class), output_tensor.shape\n",
    "summary(vit_h, input_size=(3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd0f0ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientMultiHeadAtt(nn.Module):\n",
    "    \"\"\"Efficient Attention block\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, head=8, sr_ratio=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_head = d_model // head\n",
    "        self.head = head\n",
    "        self.sr_ratio = sr_ratio\n",
    "        \n",
    "        if self.sr_ratio > 1:\n",
    "            self.sr_mlp = nn.Linear(d_model * sr_ratio, d_model)\n",
    "        \n",
    "        \n",
    "        # We don't want to create *head* instances of Linear class\n",
    "        # so we just group it to single Linear that takes in *d_model* channels and returns *d_head* x *head* channels\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        nn.init.xavier_uniform_(self.W_q.weight)\n",
    "        \n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        nn.init.xavier_uniform_(self.W_k.weight)\n",
    "        \n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        nn.init.xavier_uniform_(self.W_v.weight)\n",
    "        \n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        nn.init.xavier_uniform_(self.W_o.weight)\n",
    "        nn.init.zeros_(self.W_o.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: a B x N x C tensor\n",
    "        \n",
    "        Annotations:\n",
    "        - B: batch size\n",
    "        - N: number of token\n",
    "        - C: number of channel\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        reduced_kv_length = N//self.sr_ratio\n",
    "        if self.sr_ratio > 1:\n",
    "            x_kv = x.reshape(B, reduced_kv_length, C*self.sr_ratio)\n",
    "            x_kv = self.sr_mlp(x_kv)\n",
    "        \n",
    "        queries = self.W_q(x) # B x N x head*d_head\n",
    "        keys = self.W_k(x_kv)\n",
    "        values = self.W_v(x_kv)\n",
    "        \n",
    "        queries = queries.reshape(B, N, self.head, self.d_head).permute(0, 2, 1, 3) # B x head x N x d_head\n",
    "        keys = keys.reshape(B, reduced_kv_length, self.head, self.d_head).permute(0, 2, 1, 3)\n",
    "        values = values.reshape(B, reduced_kv_length, self.head, self.d_head).permute(0, 2, 1, 3)\n",
    "        \n",
    "        attn = (queries @ keys.transpose(-2, -1)) / self.d_head ** 0.5\n",
    "        attn = F.softmax(attn, dim=-1) # B x head x N x N\n",
    "        \n",
    "        x = attn @ values # B x head x N x h_head\n",
    "        x = x.transpose(1, 2) # B x N x head x h_head\n",
    "        x = x.reshape(B, N, C) # B x N x head*h_head - Remind: d_model = C = head*h_head\n",
    "        \n",
    "        x = self.W_o(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e1a1362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "eff_attn = EfficientMultiHeadAtt(256, 8, 4)\n",
    "attn = MultiHeadAtt(256, 8)\n",
    "\n",
    "eff_list = []\n",
    "attn_list = []\n",
    "\n",
    "for s in range(20):\n",
    "    x = torch.randn(size=(1, 512 * (s+1), 256))\n",
    "    start = time.time()\n",
    "    eff_attn(x)\n",
    "    end = time.time()\n",
    "\n",
    "    eff_time = end - start\n",
    "    eff_list.append(eff_time)\n",
    "\n",
    "    start = time.time()\n",
    "    attn(x)\n",
    "    end = time.time()\n",
    "\n",
    "    attn_time = end - start\n",
    "    attn_list.append(attn_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6a48eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xs = [512*(i+1) for i in range(20)]\n",
    "plt.plot(xs, eff_list, label=\"Efficient MSA\")\n",
    "plt.plot(xs, attn_list, label=\"MSA\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.xlabel(\"Input size\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370657d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
