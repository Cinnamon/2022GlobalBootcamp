{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Stemming + Lemmatise Ans.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E87E17O0q0B",
        "outputId": "dd6996a6-2c16-426e-c3e4-ff6dff4cf380"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "## resource:https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
        "\n",
        "## Create stemmer & Lemmatizer\n",
        "stemmer=PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer() "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhUP_bG00q0E"
      },
      "source": [
        "### example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlGsOs8C0q0G",
        "outputId": "6c407921-4def-4cbf-ce52-8c221f0bb3be"
      },
      "source": [
        "print('Stemming amusing : {}'.format(stemmer.stem('amusing')))\n",
        "print('lemmatization amusing : {}'.format(lemmatizer.lemmatize('amusing',pos = 'v')))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming amusing : amus\n",
            "lemmatization amusing : amuse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkXqB8Gw0q0H"
      },
      "source": [
        "### Use tokenize + stemming to obtain the root of every word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8L7OSKK0q0H",
        "outputId": "542dcc0a-c379-46b8-a5f3-cd3a091d1f78"
      },
      "source": [
        "# Define the sentence to be lemmatized\n",
        "sentence = \"We went out often, hiding from sight, desperately searching for food.\"\n",
        "\n",
        "# Tokenize: Split the sentence into words\n",
        "word_list = nltk.word_tokenize(sentence)\n",
        "print(word_list)\n",
        "#> ['We', 'went', 'out', 'often', ',', 'hiding', 'from', 'sight', ',', 'desperately', 'searching', 'for', 'food', '.']\n",
        "\n",
        "stemming_output = ' '.join([stemmer.stem(w) for w in word_list])\n",
        "print(stemming_output)\n",
        "#> We went out often , hide from sight , desper search for food."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['We', 'went', 'out', 'often', ',', 'hiding', 'from', 'sight', ',', 'desperately', 'searching', 'for', 'food', '.']\n",
            "we went out often , hide from sight , desper search for food .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXHaU2-_0q0I"
      },
      "source": [
        "### Use tokenize + lemmatize to obtain the lemma of every word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CubwVLP_0q0I",
        "outputId": "ef46d461-815b-4e28-81cc-ce1220eec224"
      },
      "source": [
        "# Define the sentence to be lemmatized\n",
        "sentence = \"We went out often, hiding from sight, desperately searching for food.\"\n",
        "\n",
        "# Tokenize: Split the sentence into words\n",
        "word_list = nltk.word_tokenize(sentence)\n",
        "print(word_list)\n",
        "#> ['We', 'went', 'out', 'often', ',', 'hiding', 'from', 'sight', ',', 'desperately', 'searching', 'for', 'food', '.']\n",
        "\n",
        "# Lemmatize list of words and join\n",
        "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
        "print(lemmatized_output)\n",
        "#> We went out often , hiding from sight , desperately searching for food ."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['We', 'went', 'out', 'often', ',', 'hiding', 'from', 'sight', ',', 'desperately', 'searching', 'for', 'food', '.']\n",
            "We went out often , hiding from sight , desperately searching for food .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP2smN_50q0J"
      },
      "source": [
        "### Sometimes the lemma of a word might change depending to it's Part Of Speech."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PellPA0n0q0J",
        "outputId": "5750ec22-cb5e-4903-96cd-39d1dfb3b872"
      },
      "source": [
        "print('lemmatization amusing : {}'.format(lemmatizer.lemmatize('amusing',pos = 'v'))) ##動詞\n",
        "print('lemmatization amusing : {}'.format(lemmatizer.lemmatize('amusing',pos = 'a'))) ##形容詞"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lemmatization amusing : amuse\n",
            "lemmatization amusing : amusing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ5YKRM50q0K"
      },
      "source": [
        "### Use pos_tag + lemmatize to obtain the lemma of every word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eK6kzq54-HF",
        "outputId": "235a50bb-2c22-48f7-cf25-925ad10a6b4c"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYisAzU30q0K"
      },
      "source": [
        "# Lemmatize with POS Tag\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"將pos_tag結果mapping到lemmatizer中pos的格式\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU6Rg_b60q0K",
        "outputId": "079655dc-202f-4511-c90b-fddba08dbbf5"
      },
      "source": [
        "word = 'using'\n",
        "print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "use\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4klEYOz0q0L"
      },
      "source": [
        "### Lemmatize every word in the sentence then add POS tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzaiuRTp0q0L",
        "outputId": "8fcee03b-7ac8-4f2e-deaa-a676e1c3430a"
      },
      "source": [
        "sentence = \"We went out often, hiding from sight, desperately searching for food.\"\n",
        "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])\n",
        "#> ['We', 'go', 'out', 'often', ',', 'hiding', 'from', 'sight', ',', 'desperately', 'search', 'for', 'food', '.']"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['We', 'go', 'out', 'often', ',', 'hiding', 'from', 'sight', ',', 'desperately', 'search', 'for', 'food', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIMqmPJI0q0M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}