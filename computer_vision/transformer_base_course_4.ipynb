{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/attn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAtt(nn.Module):\n",
    "    \"\"\"Basic attention block\n",
    "    \n",
    "    This is a simplified version referenced from: \n",
    "    https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py#L178-L202\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, head=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_head = d_model // head\n",
    "        self.head = head\n",
    "        \n",
    "        # We don't want to create *head* instances of Linear class\n",
    "        # so we just group it to single Linear that takes in *d_model* channels and returns *d_head* x *head* channels\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        nn.init.xavier_uniform_(self.W_q.weight)\n",
    "        \n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        nn.init.xavier_uniform_(self.W_k.weight)\n",
    "        \n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        nn.init.xavier_uniform_(self.W_v.weight)\n",
    "        \n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        nn.init.xavier_uniform_(self.W_o.weight)\n",
    "        nn.init.zeros_(self.W_o.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: a B x N x C tensor\n",
    "        \n",
    "        Annotations:\n",
    "        - B: batch size\n",
    "        - N: number of token\n",
    "        - C: number of channel\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        queries = self.W_q(x) # B x N x head*d_head\n",
    "        keys = self.W_k(x)\n",
    "        values = self.W_v(x)\n",
    "        \n",
    "        queries = queries.reshape(B, N, self.head, self.d_head).permute(0, 2, 1, 3) # B x head x N x d_head\n",
    "        keys = keys.reshape(B, N, self.head, self.d_head).permute(0, 2, 1, 3)\n",
    "        values = values.reshape(B, N, self.head, self.d_head).permute(0, 2, 1, 3)\n",
    "        \n",
    "        attn = (queries @ keys.transpose(-2, -1)) / self.d_head ** 0.5\n",
    "        attn = F.softmax(attn, dim=-1) # B x head x N x N\n",
    "        \n",
    "        x = attn @ values # B x head x N x h_head\n",
    "        x = x.transpose(1, 2) # B x N x head x h_head\n",
    "        x = x.reshape(B, N, C) # B x N x head*h_head - Remind: d_model = C = head*h_head\n",
    "        \n",
    "        x = self.W_o(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/vit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \"\"\"MLP or Feed forward network used in attention blocks\"\"\"\n",
    "    def __init__(self, d_in_out, d_hidden):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_in_out, d_hidden)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        \n",
    "        self.fc2 = nn.Linear(d_hidden, d_in_out)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: a B x N x C tensor\n",
    "        \n",
    "        Annotations:\n",
    "        - B: batch size\n",
    "        - N: number of token\n",
    "        - C: number of channel\n",
    "        \"\"\"\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Basic building block of ViT\"\"\"\n",
    "    def __init__(self, d_model, head, d_ff_hid):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_norm = nn.LayerNorm(d_model)\n",
    "        self.multi_attn = MultiHeadAtt(d_model, head)\n",
    "        \n",
    "        self.ff_norm = nn.LayerNorm(d_model)\n",
    "        self.ff = FFN(d_model, d_ff_hid)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: a B x N x C tensor\n",
    "        \n",
    "        Annotations:\n",
    "        - B: batch size\n",
    "        - N: number of token\n",
    "        - C: number of channel\n",
    "        \"\"\"\n",
    "        x_res = x\n",
    "        x = self.input_norm(x)\n",
    "        z = self.multi_attn(x) + x_res\n",
    "        \n",
    "        z_res = z\n",
    "        z = self.ff_norm(z)\n",
    "        z = self.ff(z) + z_res\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"A layer that splits image into patches and using a CNN to compute embedding feature of each patch\"\"\"\n",
    "    def __init__(self, img_size, patch_size, img_c, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_size = img_size if isinstance(img_size, tuple) else (img_size, img_size)\n",
    "        self.patch_size = patch_size if isinstance(patch_size, tuple) else (patch_size, patch_size)\n",
    "        \n",
    "        self.grid_size = (self.img_size[0] // self.patch_size[0], self.img_size[1] // self.patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        \n",
    "        self.conv = nn.Conv2d(img_c, d_model, kernel_size=self.patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: a B x C x H x W image tensor\n",
    "        \n",
    "        Annotations:\n",
    "        - B: batch size\n",
    "        - C: number of channel\n",
    "        - H: height\n",
    "        - W: width\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        \n",
    "        x = self.conv(x) # B x d_model x grid_H x grid_W\n",
    "        x = torch.flatten(x, 2) # B x d_model x N\n",
    "        x = x.transpose(1, 2) # B x N x d_model\n",
    "        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \"\"\"A skeleton of a typical ViT model\"\"\"\n",
    "    def __init__(self, img_size, patch_size, img_c, d_model, num_class, encoders):\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        self.patch_embeder = PatchEmbed(img_size, patch_size, img_c, d_model)\n",
    "        \n",
    "        num_patches = self.patch_embeder.num_patches\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, d_model))\n",
    "        \n",
    "        self.encoders = encoders\n",
    "        self.mlp_head = nn.Linear(d_model, num_class)\n",
    "        self.cls_morm = nn.LayerNorm(num_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: a B x C x H x W image tensor\n",
    "        \n",
    "        Annotations:\n",
    "        - B: batch size\n",
    "        - C: number of channel\n",
    "        - H: height\n",
    "        - W: width\n",
    "        \"\"\"\n",
    "        x_embed = self.patch_embeder(x)\n",
    "        x_embed = torch.cat([x_embed, self.cls_token], dim=1)\n",
    "        x_embed = x_embed + self.pos_embed\n",
    "        \n",
    "        x_transformed = self.encoders(x_embed)\n",
    "        cls_logits = self.mlp_head(x_transformed)\n",
    "        cls_logits = self.cls_morm(cls_logits)\n",
    "        return cls_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeiT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CaiT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/cait.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassAttention(MultiHeadAtt):\n",
    "    \"\"\"New module of CaiT, ClassAttention only utilize the tokens to compute the final class for class token\n",
    "    \n",
    "    This is the simplified version referenced from:\n",
    "    https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/cait.py#L74-L106\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: a B x N x C tensor\n",
    "        \n",
    "        Annotations:\n",
    "        - B: batch size\n",
    "        - N: number of token\n",
    "        - C: number of channel\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        queries = self.W_q(x[:,0]) # Only takes class token as query\n",
    "        keys = self.W_k(x) # B x N x head*d_head\n",
    "        values = self.W_v(x)\n",
    "        \n",
    "        queries = queries.reshape(B, 1, self.head, self.d_head).permute(0, 2, 1, 3) # B x head x N x d_head\n",
    "        keys = keys.reshape(B, N, self.head, self.d_head).permute(0, 2, 1, 3)\n",
    "        values = values.reshape(B, N, self.head, self.d_head).permute(0, 2, 1, 3)\n",
    "        \n",
    "        attn = (queries @ keys.transpose(-2, -1)) / self.d_head ** 0.5\n",
    "        attn = F.softmax(attn, dim=-1) # B x head x 1 x N\n",
    "        \n",
    "        x = attn @ values # B x head x 1 x h_head\n",
    "        x = x.transpose(1, 2) # B x 1 x head x h_head\n",
    "        x = x.reshape(B, 1, C) # B x 1 x head*h_head - Remind: d_model = C = head*h_head\n",
    "        \n",
    "        x = self.W_o(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerScale(nn.Module):\n",
    "    \"\"\"Proposed layer of CaiT to make the optimization more stable\"\"\"\n",
    "    def __init__(self, n_channel, agg_block, init_val=1e-4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gamma = nn.Parameter(init_val * torch.ones((n_channel)))\n",
    "        self.layer_norm = nn.LayerNorm(n_channel)\n",
    "        self.agg_block = agg_block\n",
    "    \n",
    "    def forward(self, x, x_res):\n",
    "        return x_res + self.gamma * self.agg_block(self.layer_norm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CABlock(nn.Module):\n",
    "    \"\"\"Basic block of CaiT, utilize at the end of the network\"\"\"\n",
    "    def __init__(self, d_model, head, mlp_hidden):\n",
    "        self.cls_attn = LayerScale(n_channel=d_model, agg_block=ClassAttention(d_model, head))\n",
    "        self.mlp = LayerScale(n_channel=d_model, agg_block=FFN(d_model, mlp_hidden))\n",
    "    \n",
    "    def forward(self, x, x_cls):\n",
    "        u = torch.cat([x_cls, x], dim=1)\n",
    "        x_cls = self.cls_attn(u, x_cls)\n",
    "        x_cls = self.mlp(x_cls, x_cls)\n",
    "        return x_cls\n",
    "\n",
    "class SABlock(nn.Module):\n",
    "    \"\"\"Basic block of CaiT, utilize at the beginning of the network\"\"\"\n",
    "    def __init__(self, d_model, head, mlp_hidden):\n",
    "        self.attn = LayerScale(n_channel=d_model, agg_block=MultiHeadAtt(d_model, head))\n",
    "        self.mlp = LayerScale(n_channel=d_model, agg_block=FFN(d_model, mlp_hidden))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.attn(x, x)\n",
    "        x = self.mlp(x, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/volo_attn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlookAttention(nn.Module):\n",
    "    \"\"\"Custom module VOLO\n",
    "    \n",
    "    This is the simplified version referenced from:\n",
    "    https://github.com/sail-sg/volo/blob/main/models/volo.py#L45-L100\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, head, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.head_dim = d_model // head\n",
    "        self.head = head\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "        \n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.attn = nn.Linear(d_model, head * kernel_size**4, bias=False)\n",
    "        \n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.unfold = nn.Unfold(kernel_size=kernel_size, padding=self.padding)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        \n",
    "        attn_map = self.attn(x).reshape(B, H * W, self.head, self.kernel_size**2, self.kernel_size**2)\n",
    "        attn_map = attn_map.permute(0, 2, 1, 3, 4) / self.head_dim**0.5\n",
    "        attn_map = F.softmax(attn_map, dim=-1) # B x head x N x k^2 x k^2\n",
    "        \n",
    "        v = self.W_v(x).permute(0, 3, 1, 2) # B x head*head_dim x H x W\n",
    "        unfolded_v = self.unfold(v) # B x head*head_dim x H x W x k^2\n",
    "        unfolded_v = unfolded_v.reshape(B, self.head, -1, self.kernel_size**2, H*W) # B x head x head_dim x k^2 x N\n",
    "        unfolded_v = unfolded_v.permute(0, 1, 4, 3, 2) # B x head x N x k^2 x head_dim\n",
    "        \n",
    "        agg_val = attn_map @ unfolded_v #  B x head x N x k^2 x head_dim\n",
    "        agg_val = agg_val.permute(0, 1, 4, 3, 2).reshape(B, C, self.kernel_size**2, H*W) # B x d_model x k^2 x N\n",
    "        folded_val = F.fold(agg_val, output_size=(H, W),\n",
    "                            kernel_size=self.kernel_size, padding=self.padding) # B x d_model x H x W\n",
    "        folded_val = folded_val.permute(0, 2, 3, 1)\n",
    "        \n",
    "        prj_val = self.proj(folded_val)\n",
    "        \n",
    "        return prj_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Outlooker(nn.Module):\n",
    "    \"\"\"Basic block of VOLO that utilize at the beginning of the network\"\"\"\n",
    "    def __init__(self, d_model, d_mlp_hidden, head, kernel_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm_attn = nn.LayerNorm(d_model)\n",
    "        self.outlook_attn = OutlookAttention(d_model, head, kernel_size)\n",
    "        \n",
    "        self.layer_norm_mlp = nn.LayerNorm(d_model)\n",
    "        self.mlp = FFN(d_model, d_mlp_hidden)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.outlook_attn(self.layer_norm_attn(x))\n",
    "        x = x + self.mlp(self.layer_norm_mlp(x))\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
