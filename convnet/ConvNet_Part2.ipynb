{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_planes, eps=0.001)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet - Depthwise separable convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/mobilenet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeparable(nn.Module):\n",
    "    def __init__(self, nin, nout):\n",
    "        super(DepthwiseSeparable, self).__init__()\n",
    "        self.depthwise = nn.Sequential(nn.Conv2d(nin, nin, kernel_size=3, padding=1, groups=nin), \n",
    "                                       nn.BatchNorm2d(nin), nn.ReLU6(inplace=True))\n",
    "        self.pointwise = nn.Sequential(nn.Conv2d(nin, nout, kernel_size=1), \n",
    "                                       nn.BatchNorm2d(nout), nn.ReLU6(inplace=True))\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "    \n",
    "class General(nn.Module):\n",
    "    def __init__(self, nin, nout):\n",
    "        super(General, self).__init__()\n",
    "        self.depthwise = nn.Sequential(nn.Conv2d(nin, nin, kernel_size=3, padding=1), \n",
    "                                       nn.BatchNorm2d(nin), nn.ReLU6(inplace=True))\n",
    "        self.pointwise = nn.Sequential(nn.Conv2d(nin, nout, kernel_size=1), \n",
    "                                       nn.BatchNorm2d(nout), nn.ReLU6(inplace=True))\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DepthwiseSeparable params : 1504\n",
      "General params : 10432\n"
     ]
    }
   ],
   "source": [
    "model = DepthwiseSeparable(32, 32)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(f\"DepthwiseSeparable params : {params}\")\n",
    "\n",
    "model = General(32, 32)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(f\"General params : {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/inverted.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/inverted_detail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, nin, nout, stride, expansion_ratio=4):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        expand_ = expansion_ratio * nin\n",
    "        self.stride = stride\n",
    "        self.expansion = nn.Sequential(nn.Conv2d(nin, expand_, kernel_size=(1, 1), padding=0, groups=1),\n",
    "                                       nn.ReLU6(inplace=True))\n",
    "        self.depthwise = nn.Sequential(nn.Conv2d(expand_, expand_, kernel_size=(3, 3),\n",
    "                                                 stride=stride, padding=1, groups=expand_),\n",
    "                                       nn.ReLU6(inplace=True))\n",
    "        self.pointwise = nn.Conv2d(expand_, nout, kernel_size=(1, 1), padding=0, groups=1)\n",
    "        \n",
    "        if stride == 1:\n",
    "            if (nin != nout):\n",
    "                self.map = nn.Conv2d(nin, nout, kernel_size=(1, 1), stride=stride, padding=0, bias=False)\n",
    "            else:\n",
    "                self.map = nn.Identity()\n",
    "    def forward(self, x):\n",
    "        out = self.expansion(x)\n",
    "        out = self.depthwise(out)\n",
    "        out = self.pointwise(out)\n",
    "        \n",
    "        if self.stride == 1:\n",
    "            identity = self.map(x)\n",
    "            out += identity\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "features = torch.randn(batch_size, 32, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape : torch.Size([4, 32, 32, 32])\n",
      "Output shape : torch.Size([4, 32, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "model = InvertedResidual(32, 32, 2, 4)\n",
    "output = model(features)\n",
    "print(f\"Input shape : {features.shape}\\nOutput shape : {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SqueezeExcitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/se.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_divisible(v: float, divisor: int, min_value=None) -> int:\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    # Implemented as described at Figure 4 of the MobileNetV3 paper\n",
    "    def __init__(self, input_channels: int, squeeze_factor: int = 4):\n",
    "        super().__init__()\n",
    "        squeeze_channels = _make_divisible(input_channels // squeeze_factor, 8)\n",
    "        self.fc1 = nn.Conv2d(input_channels, squeeze_channels, 1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(squeeze_channels, input_channels, 1)\n",
    "\n",
    "    def _scale(self, input: torch.Tensor, inplace: bool) -> torch.Tensor:\n",
    "        scale = F.adaptive_avg_pool2d(input, 1)\n",
    "        scale = self.fc1(scale)\n",
    "        scale = self.relu(scale)\n",
    "        scale = self.fc2(scale)\n",
    "        return torch.sigmoid(scale)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        scale = self._scale(input, True)\n",
    "        return scale * input\n",
    "    \n",
    "    \n",
    "class SEResidualBlockV2(nn.Module):\n",
    "    def __init__(self, in_planes: int, out_planes: int, stride: int = 1, se=True):\n",
    "        super().__init__()\n",
    "        self.se = se\n",
    "        squeeze = out_planes // 4\n",
    "        self.conv1 = BasicConv2d(in_planes, squeeze, (1, 1), 1, 0) ## squeeze\n",
    "        self.conv2 = BasicConv2d(squeeze, squeeze, (3, 3), stride=stride, padding=1) ## squeeze\n",
    "        self.conv3 = nn.Conv2d(squeeze, out_planes, (1, 1), 1, 0) ## expand\n",
    "        \n",
    "        if self.se:\n",
    "            self.squeeze_excitation = SqueezeExcitation(out_planes)\n",
    "        if (in_planes != out_planes) or (stride != 1):\n",
    "            self.map = nn.Conv2d(in_planes, out_planes, kernel_size=(1, 1), stride=stride, padding=0, bias=False)\n",
    "        else:\n",
    "            self.map = nn.Identity()\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = self.map(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        if self.se:\n",
    "            x = self.squeeze_excitation(x)\n",
    "        x += identity\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SEResidualBlockV2(64, 64, se=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape : torch.Size([4, 64, 32, 32])\n",
      "Output shape : torch.Size([4, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "features = torch.randn(batch_size, 64, 32, 32)\n",
    "output = model(features)\n",
    "print(f\"Input shape : {features.shape}\\nOutput shape : {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/swish.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/silu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiLU(nn.Module):  #The SiLU function is also known as the swish function - export-friendly version of nn.SiLU() \n",
    "    def forward(self, x): \n",
    "        return x * torch.sigmoid(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### memory-efficient version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = torch.nn.Sigmoid()\n",
    "class SwishCustomized(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * sigmoid(i)\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_variables[0]\n",
    "        sigmoid_i = sigmoid(i)\n",
    "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
    "\n",
    "swish = SwishCustomized.apply\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return swish(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeExcitation(nn.Module):\n",
    "    # Implemented as described at Figure 4 of the MobileNetV3 paper\n",
    "    def __init__(self, input_channels: int, squeeze_factor: int = 4):\n",
    "        super().__init__()\n",
    "        squeeze_channels = _make_divisible(input_channels // squeeze_factor, 8)\n",
    "        self.fc1 = nn.Conv2d(input_channels, squeeze_channels, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(squeeze_channels, input_channels, 1)\n",
    "\n",
    "    def _scale(self, input: torch.Tensor, inplace: bool) -> torch.Tensor:\n",
    "        scale = F.adaptive_avg_pool2d(input, 1)\n",
    "        scale = self.fc1(scale)\n",
    "        scale = self.relu(scale)\n",
    "        scale = self.fc2(scale)\n",
    "        return torch.sigmoid(scale)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        scale = self._scale(input, True)\n",
    "        return scale * input\n",
    "    \n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, nin, nout, stride, expansion_ratio=4, se=True):\n",
    "        super(MBConv, self).__init__()\n",
    "        expand_ = expansion_ratio * nout\n",
    "        self.stride = stride\n",
    "        self.se = se\n",
    "        self.expansion = nn.Sequential(nn.Conv2d(nin, expand_, kernel_size=1), \n",
    "                                       nn.BatchNorm2d(expand_), SiLU())\n",
    "        \n",
    "        self.depthwise = nn.Sequential(nn.Conv2d(expand_, expand_, kernel_size=3, padding=1, stride=stride, groups=nin), \n",
    "                                       nn.BatchNorm2d(expand_), SiLU())\n",
    "        \n",
    "        self.pointwise = nn.Sequential(nn.Conv2d(expand_, nout, kernel_size=1), \n",
    "                                       nn.BatchNorm2d(nout))\n",
    "        \n",
    "        if self.se:\n",
    "            self.squeeze_excitation = SqueezeExcitation(expand_)\n",
    "        \n",
    "        if stride == 1:\n",
    "            if (nin != nout):\n",
    "                self.map = nn.Conv2d(nin, nout, kernel_size=(1, 1), stride=stride, padding=0, bias=False)\n",
    "            else:\n",
    "                self.map = nn.Identity()\n",
    "    def forward(self, x):\n",
    "        out = self.expansion(x)\n",
    "        out = self.depthwise(out)\n",
    "        \n",
    "        if self.se:\n",
    "            out = self.squeeze_excitation(out)\n",
    "            \n",
    "        out = self.pointwise(out)\n",
    "            \n",
    "        if self.stride == 1:\n",
    "            identity = self.map(x)\n",
    "            out += identity\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = False\n",
    "if demo:\n",
    "    model = nn.Conv2d(10, 10, 1)\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    print(f\"params : {params}\")\n",
    "\n",
    "    model = nn.Linear(10, 10)\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    print(f\"params : {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBConv(64, 64, 1, 6, se=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape : torch.Size([4, 64, 32, 32])\n",
      "Output shape : torch.Size([4, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "features = torch.randn(batch_size, 64, 32, 32)\n",
    "output = model(features)\n",
    "print(f\"Input shape : {features.shape}\\nOutput shape : {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### practice\n",
    "* Build MobileNetV3 Block\n",
    "* Replace sigmoid / swish to h-sigmoid / h-swish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/hswish.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/h-swish.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HSigmoid(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return (F.relu6(x + 3, inplace=True) / 6)\n",
    "            \n",
    "class HSwish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hsigmoid = HSigmoid()\n",
    "    def forward(self, x):\n",
    "        return x * self.hsigmoid(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GhostNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/ghost.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class GhostModule(nn.Module):\n",
    "    def __init__(self, inp, oup, kernel_size=1, ratio=2, dw_size=3, stride=1, relu=True):\n",
    "        super(GhostModule, self).__init__()\n",
    "        self.oup = oup\n",
    "        init_channels = math.ceil(oup / ratio)\n",
    "        new_channels = init_channels*(ratio-1)\n",
    "\n",
    "        self.primary_conv = nn.Sequential(\n",
    "            nn.Conv2d(inp, init_channels, kernel_size,\n",
    "                      stride, kernel_size//2, bias=False),\n",
    "            nn.BatchNorm2d(init_channels),\n",
    "            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n",
    "        )\n",
    "\n",
    "        self.cheap_operation = nn.Sequential(\n",
    "            nn.Conv2d(init_channels, new_channels, dw_size, 1,\n",
    "                      dw_size//2, groups=init_channels, bias=False),\n",
    "            nn.BatchNorm2d(new_channels),\n",
    "            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n",
    "        ) ## 1*1 depthwise convolution\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.primary_conv(x)\n",
    "        x2 = self.cheap_operation(x1)\n",
    "        out = torch.cat([x1, x2], dim=1)\n",
    "        return out[:, :self.oup, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GhostModule(64, 64, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape : torch.Size([4, 64, 32, 32])\n",
      "Output shape : torch.Size([4, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "features = torch.randn(batch_size, 64, 32, 32)\n",
    "output = model(features)\n",
    "print(f\"Input shape : {features.shape}\\nOutput shape : {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NFNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we remove batch normalization layer?\n",
    "\n",
    "#### Advantage of BN\n",
    "    * Batch normalization downscales the residual branch.\n",
    "    * Batch normalization allows efficient large-batch training \n",
    "    * Smoothens the loss landscape\n",
    "    * Avoid elimination singularities\n",
    "    * Batch normalization eliminates mean-shift\n",
    "#### Disadvantage of BN\n",
    "    * Expensive computational primitive.\n",
    "    * Introduces a discrepancy between the be- haviour of the model during training and at inference time.\n",
    "    * Batch normalization breaks the independence between training examples in the minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Towards Removing Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptive Gradient Clipping\n",
    "* Batch normalization allows efficient large-batch training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/AGC.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## source : https://github.com/vballoli/nfnets-pytorch\n",
    "def unitwise_norm(x: torch.Tensor):\n",
    "    if x.ndim <= 1:\n",
    "        dim = 0\n",
    "        keepdim = False\n",
    "    elif x.ndim in [2, 3]:\n",
    "        dim = 0\n",
    "        keepdim = True\n",
    "    elif x.ndim == 4:\n",
    "        dim = [1, 2, 3]\n",
    "        keepdim = True\n",
    "    else:\n",
    "        raise ValueError('Wrong input dimensions')\n",
    "\n",
    "    return torch.sum(x**2, dim=dim, keepdim=keepdim) ** 0.5\n",
    "\n",
    "def example():\n",
    "    param_norm = torch.max(unitwise_norm(p.detach()), torch.tensor(0.001).to(p.device)) ## ||W(l,i)||*F\n",
    "    grad_norm = unitwise_norm(p.grad.detach()) ## ||G(l,i)||*F\n",
    "    max_norm = param_norm * self.clipping\n",
    "\n",
    "    trigger = grad_norm > max_norm ## ||G(l,i)||*F > ||W(l,i)||*F * self.clipping\n",
    "\n",
    "    clipped_grad = p.grad * (max_norm / torch.max(grad_norm,torch.tensor(1e-6).to(grad_norm.device)))\n",
    "    p.grad.detach().data.copy_(torch.where(trigger, clipped_grad, p.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled Weight Standardization\n",
    "* In addition, Brock et al. (2021) prevent the emergence of a **mean-shift** in the hidden activations by introducing Scaled Weight Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/sws.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledStdConv2d(nn.Conv2d):\n",
    "    \"\"\"Conv2d layer with Scaled Weight Standardization.\n",
    "    Paper: `Characterizing signal propagation to close the performance gap in unnormalized ResNets` -\n",
    "        https://arxiv.org/abs/2101.08692\n",
    "    Adapted from timm: https://github.com/rwightman/pytorch-image-models/blob/4ea593196414684d2074cbb81d762f3847738484/timm/models/layers/std_conv.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1,\n",
    "                 bias=True, gain=True, gamma=1.0, eps=1e-5, use_layernorm=False):\n",
    "        super().__init__(\n",
    "            in_channels, out_channels, kernel_size, stride=stride,\n",
    "            padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.gain = nn.Parameter(torch.ones(\n",
    "            self.out_channels, 1, 1, 1)) if gain else None\n",
    "        # gamma * 1 / sqrt(fan-in)\n",
    "        # Fan-in: is a term that defines the maximum number of inputs that a system can accept.\n",
    "        self.scale = gamma * self.weight[0].numel() ** -0.5 ## numel() : number of element -> 1 / N**0.5\n",
    "        self.eps = eps ** 2 if use_layernorm else eps\n",
    "        # experimental, slightly faster/less GPU memory use\n",
    "        self.use_layernorm = use_layernorm\n",
    "\n",
    "    def get_weight(self):\n",
    "        if self.use_layernorm:\n",
    "            weight = self.scale * \\\n",
    "                F.layer_norm(self.weight, self.weight.shape[1:], eps=self.eps)\n",
    "        else:\n",
    "            mean = torch.mean(\n",
    "                self.weight, dim=[1, 2, 3], keepdim=True)\n",
    "            std = torch.std(\n",
    "                self.weight, dim=[1, 2, 3], keepdim=True, unbiased=False)\n",
    "            weight = self.scale * (self.weight - mean) / (std + self.eps)\n",
    "        if self.gain is not None:\n",
    "            weight = weight * self.gain ## scaled\n",
    "        return weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.conv2d(x, self.get_weight(), self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizer-Free Residual Block\n",
    "* Batch normalization downscales the residual branch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/free.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from functools import partial\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1, base_conv: nn.Conv2d = ScaledStdConv2d) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return base_conv(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        alpha: float = 0.2,\n",
    "        beta: float = 1.0,\n",
    "        base_conv: nn.Conv2d = ScaledStdConv2d\n",
    "    ) -> None:\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\n",
    "                'BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\n",
    "                \"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride, base_conv=base_conv)\n",
    "        \n",
    "        self.act = nn.LeakyReLU()\n",
    "        self.conv2 = conv3x3(planes, planes, base_conv=base_conv)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "        \n",
    "        out = x * self.beta\n",
    "\n",
    "        out = self.conv1(out)\n",
    "        out = self.act(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out *= self.alpha\n",
    "        out += identity\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicBlock(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "output = model(torch.randn(1, 3, 32, 32))\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNetV2\n",
    "* Combination of training-aware neural architecture search (NAS) and scaling to improve both training speed and parameter efficiency.\n",
    "* Depthwise convolutions are slow in **early layers** but effective in later stages.\n",
    "* Gradually increase the learning difficulty with larger im- age sizes and stronger regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/fused.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeExcitation(nn.Module):\n",
    "    # Implemented as described at Figure 4 of the MobileNetV3 paper\n",
    "    def __init__(self, input_channels: int, squeeze_factor: int = 4):\n",
    "        super().__init__()\n",
    "        squeeze_channels = _make_divisible(input_channels // squeeze_factor, 8)\n",
    "        self.fc1 = nn.Conv2d(input_channels, squeeze_channels, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(squeeze_channels, input_channels, 1)\n",
    "\n",
    "    def _scale(self, input: torch.Tensor, inplace: bool) -> torch.Tensor:\n",
    "        scale = F.adaptive_avg_pool2d(input, 1)\n",
    "        scale = self.fc1(scale)\n",
    "        scale = self.relu(scale)\n",
    "        scale = self.fc2(scale)\n",
    "        return torch.sigmoid(scale)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        scale = self._scale(input, True)\n",
    "        return scale * input\n",
    "    \n",
    "class FusedMBConv(nn.Module):\n",
    "    def __init__(self, nin, nout, stride, expansion_ratio=4, se=True):\n",
    "        super(FusedMBConv, self).__init__()\n",
    "        expand_ = expansion_ratio * nout\n",
    "        self.stride = stride\n",
    "        self.se = se\n",
    "        self.expansion = nn.Sequential(nn.Conv2d(nin, expand_, kernel_size=3, padding=1), \n",
    "                                       nn.BatchNorm2d(expand_), SiLU())\n",
    "        \n",
    "        if self.se:\n",
    "            self.squeeze_excitation = SqueezeExcitation(expand_)\n",
    "            \n",
    "        self.pointwise = nn.Sequential(nn.Conv2d(expand_, nout, kernel_size=1), \n",
    "                                       nn.BatchNorm2d(nout))\n",
    "        \n",
    "        \n",
    "        if stride == 1:\n",
    "            if (nin != nout):\n",
    "                self.map = nn.Conv2d(nin, nout, kernel_size=(1, 1), stride=stride, padding=0, bias=False)\n",
    "            else:\n",
    "                self.map = nn.Identity()\n",
    "    def forward(self, x):\n",
    "        out = self.expansion(x)\n",
    "        \n",
    "        if self.se:\n",
    "            out = self.squeeze_excitation(out)\n",
    "            \n",
    "        out = self.pointwise(out)\n",
    "            \n",
    "        if self.stride == 1:\n",
    "            identity = self.map(x)\n",
    "            out += identity\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FusedMBConv(64, 64, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape : torch.Size([4, 64, 32, 32])\n",
      "Output shape : torch.Size([4, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "features = torch.randn(batch_size, 64, 32, 32)\n",
    "output = model(features)\n",
    "print(f\"Input shape : {features.shape}\\nOutput shape : {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice : Train your model on Cifar10\n",
    "* reference : https://github.com/jeff52415/Tips-for-improving-your-neural-network-pytorch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic_env",
   "language": "python",
   "name": "basic_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
